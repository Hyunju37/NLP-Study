{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습 3.1 Text Classification with RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['부산', '행', '때문', '너무', '기대하고', '봤'], ['한국', '좀비', '영화', '어색하지', '않게', '만들어졌', '놀랍']]\n"
     ]
    }
   ],
   "source": [
    "with open('./Korean_movie_reviews_2016.txt', encoding='utf-8') as f:\n",
    "    docs = [doc.strip().split('\\t') for doc in f]\n",
    "    docs = [(doc[0], int(doc[1])) for doc in docs if len(doc) == 2]\n",
    "    texts, labels = zip(*docs)\n",
    "    \n",
    "words_list = [doc.strip().split() for doc in texts]\n",
    "print(words_list[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = []\n",
    "for words in words_list:\n",
    "    total_words.extend(words)\n",
    "    \n",
    "from collections import Counter\n",
    "c = Counter(total_words)\n",
    "\n",
    "max_features = 10000\n",
    "common_words = [ word for word, count in c.most_common(max_features)]\n",
    "# 빈도를 기준으로 상위 10000개의 단어들만 선택\n",
    "\n",
    "# 각 단어에 대해서 index 생성하기\n",
    "words_dic = {}\n",
    "for index, word in enumerate(common_words):\n",
    "    words_dic[word]=index+1\n",
    "    \n",
    "# 각 문서를 상위 10000개 단어들에 대해서 index 번호로 표현하기\n",
    "filtered_indexed_words = []\n",
    "for review in words_list:\n",
    "    indexed_words=[]\n",
    "    for word in review:\n",
    "        try:\n",
    "            indexed_words.append(words_dic[word])\n",
    "        except:\n",
    "            pass\n",
    "    filtered_indexed_words.append(indexed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loader for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# X input padding\n",
    "max_len = 40\n",
    "X = sequence.pad_sequences(filtered_indexed_words, maxlen=max_len)\n",
    "# y to one-hot category labeling\n",
    "y_one_hot = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132307\n",
      "33077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Build\n",
    "실습문제 3.1.1 Bidirectional LSTM을 구현하여 모델링을 완성하시오\n",
    "from tensorflow.kears import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, 40, 64)            640064    \n",
      "                                                                 \n",
      " bidirectional_34 (Bidirect  (None, 40, 64)            24832     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_35 (Bidirect  (None, 32)                10368     \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 675826 (2.58 MB)\n",
      "Trainable params: 675826 (2.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, AdamW, RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Embedding(max_features+1, 64, input_shape=(max_len, )))\n",
    "'''\n",
    "Bidirectional 파라미터\n",
    "merge_mode: mode by which outputs of the forward and backward RNNs will be combined.\n",
    "One of {sum/mul/concat(default)/ave/None} If None, the outputs will not be combined, they will be returned as a list.\n",
    "'''\n",
    "# return_sequence=True : whether to return the last output in the output sequence, or the full sequence\n",
    "# return_state=True : whether to return the last state in addition to the output. \n",
    "model.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True), merge_mode='concat'))\n",
    "model.add(layers.Bidirectional(layers.LSTM(16)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2, activation='softmax')) #binary classification\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "827/827 [==============================] - 85s 95ms/step - loss: 0.4264 - accuracy: 0.7995 - val_loss: 0.2766 - val_accuracy: 0.8858\n",
      "Epoch 2/5\n",
      "827/827 [==============================] - 81s 98ms/step - loss: 0.2818 - accuracy: 0.8933 - val_loss: 0.3040 - val_accuracy: 0.8908\n",
      "Epoch 3/5\n",
      "827/827 [==============================] - 91s 110ms/step - loss: 0.2591 - accuracy: 0.9030 - val_loss: 0.2628 - val_accuracy: 0.8921\n",
      "Epoch 4/5\n",
      "827/827 [==============================] - 85s 103ms/step - loss: 0.2438 - accuracy: 0.9099 - val_loss: 0.2569 - val_accuracy: 0.8947\n",
      "Epoch 5/5\n",
      "827/827 [==============================] - 88s 107ms/step - loss: 0.2304 - accuracy: 0.9154 - val_loss: 0.2517 - val_accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034/1034 [==============================] - 15s 15ms/step - loss: 0.2520 - accuracy: 0.8965\n",
      "Loss: 0.25201135873794556\n",
      "Accuracy: 0.8965141773223877\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Loss:\", test_loss)\n",
    "print(\"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 3.2 Machine Translation with Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s) # decompose string 's' into its base characters and combining characters using the Normalization Form D(NFD) scheme\n",
    "                 if unicodedata.category(c)!= 'Mn') #checks if the character's category, obtained using unicodedata.category(), is not equal to 'Mn' \n",
    "#(Mn stands for 'Mark, Nonspacing' which includes diacritic marks like accents, umlauts, etc. So this condition effecively filters out all diacritic marks from the string)\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "  # 단어와 단어 뒤에 오는 구두점(.)사이에 공백을 생성.\n",
    "    # 예시: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # 참고:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \",w)\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")을 제외한 모든 것을 공백으로 대체합니다.\n",
    "  # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "  w = w.strip()\n",
    "\n",
    "  # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해서\n",
    "    # 문장에 start와 end 토큰을 추가합니다.\n",
    "    \n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
      "<start> 의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반대의 상황이 존재하지 . 그런데 인간이 수백 명의 사람만 알고 지내는 사이가 될 기회를 갖는다고 생각해 보면 , 또 그 수백 명 중 열여 명 쯤 이하만 잘 알 수 있고 , 그리고 나서 그 열여 명 중에 한두 명만 친구가 될 수 있다면 , 그리고 또 만일 우리가 이 세상에 살고 있는 수백만 명의 사람들만 기억하고 있다면 , 딱 맞는 남자는 지구가 생겨난 이래로 딱 맞는 여자를 단 한번도 만난 적이 없을 수도 있을 거라는 사실을 쉽게 눈치챌 수 있을 거야 . <end>\n",
      "5890\n",
      "5890\n"
     ]
    }
   ],
   "source": [
    "# 1. 문장에 있는 억양을 제거합니다.\n",
    "# 2. 불필요한 문자를 제거하여 문장을 정리합니다.\n",
    "# 3. 다음과 같은 형식으로 문장의 쌍을 반환합니다: [영어, 스페인어]\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "  ens = []\n",
    "  spas = []\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  for l in lines[:num_examples]:\n",
    "    word_pairs = [preprocess_sentence(w) for w in l.split('\\t')[:2]]\n",
    "    en, spa = word_pairs\n",
    "    ens.append(en)\n",
    "    spas.append(spa)\n",
    "  return ens, spas\n",
    "\n",
    "path_to_file = \"kor.txt\"\n",
    "en, kor = create_dataset(path_to_file, None)\n",
    "\n",
    "print(en[-1])\n",
    "print(kor[-1])\n",
    "\n",
    "print(len(en))\n",
    "print(len(kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing & Padding & Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') #no characters are filtered out during tokenization\n",
    "  lang_tokenizer.fit_on_texts(lang) #fits the tokenizer on the input text 'lang'. This step updates the internal vocabulary of the tokenizer based on the words(or tokens) present in the input text\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang) #converts the text sequences in 'lang' into sequence of integers.(index according to the vocabulary learned by the previous step)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # pads the sequences of integers in 'tensor' to ensure that they all have the same length\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # 전처리된 타겟 문장과 입력 문장 쌍을 생성합니다.\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4712 4712 1178 1178\n"
     ]
    }
   ],
   "source": [
    "# 언어 데이터셋을 아래의 크기로 제한하여 훈련과 검증을 수행합니다.\n",
    "num_examples = 10000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# 타겟 텐서와 입력 텐서의 최대 길이를 계산합니다.\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "# 훈련 집합과 검증 집합을 80대 20으로 분리합니다.\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# 훈련 집합과 검증 집합의 데이터 크기를 출력합니다.\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 97\n"
     ]
    }
   ],
   "source": [
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "8 ----> 톰이\n",
      "2077 ----> 마실\n",
      "12 ----> 수\n",
      "44 ----> 있는\n",
      "70 ----> 것은\n",
      "745 ----> 오직\n",
      "329 ----> 물\n",
      "406 ----> 뿐이야\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "150 ----> water\n",
      "10 ----> is\n",
      "9 ----> the\n",
      "178 ----> only\n",
      "433 ----> thing\n",
      "5 ----> tom\n",
      "68 ----> will\n",
      "303 ----> drink\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "            \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Build for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "# calculates the number of steps per epoch based on the length of the training input tensor and the chosen batch size.\n",
    "# for defining the number of iterations per epoch during training\n",
    "embedding_dim = 128\n",
    "units = 512 # number of units in the recurrent units\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "#+1 is for accomodating the padding token.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) #drop_remainder: ensures that any remaining samples that don't fit into a full batch are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 97]), TensorShape([64, 112]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset 크기출력 ///왜 1차원???not 2차원?? dataset 오타.\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더 모델과 디코더 모델 쓰기\n",
    "어텐션(attention)을 가진 인코더-디코더 모델을 수행합니다. 어텐션(attention)은 TensorFlow Neural Machine Translation (seq2seq) tutorial에서 읽을 수 있습니다. 이 예제는 더 최신의 API 집합을 사용합니다. 이 노트북은 seq2seq 튜토리얼로부터 어텐션 방정식을 수행합니다. 아래의 다이어그램은 각각의 입력 단어가 어텐션 메커니즘에 의해 가중치가 할당된 모습입니다. 이러한 어텐션 메커니즘은 디코더가 문장에서 다음 단어를 예측하기 위해 사용됩니다. 아래의 그림과 공식은 Luong's paper에서 나온 어텐션 메커니즘의 예시입니다.\n",
    "\n",
    "![image.png](attachment:c0952e5c-9c02-49c8-9117-f3fcf6b60447.png)![image.png](attachment:c0444d39-21a8-43e4-9792-86205b74b8ed.png)\n",
    "\n",
    "이 튜토리얼은 인코더를 위해 Bahdanau 어텐션을 사용합니다. 단순화된 형태로 쓰기 전에 표기법을 아래와 같이 정의합니다:\n",
    "\n",
    "FC = 완전 연결(Dense)층\n",
    "\n",
    "EO = 인코더 결과\n",
    "\n",
    "H = 은닉 상태(hidden state)\n",
    "\n",
    "X = 디코더에 대한 입력\n",
    "그리고 다음은 슈도코드입니다:\n",
    "\n",
    "스코어(score)는 FC(tanh(FC(EO) + FC(H)))로 계산합니다.\n",
    "어텐션 가중치는 softmax(score, axis = 1)로 계산합니다. 기본적으로 소프트맥스는 마지막 축을 적용하지만 스코어(score)의 형태가 (batch_size, max_length, hidden_size)이기 때문에 첫번째 축을 적용합니다. Max_length은 입력의 길이입니다. 각각의 입력에 가중치를 할당하려고 시도하기 때문에 소프트맥스는 그 축을 적용할 수 있습니다.\n",
    "컨텍스트 벡터(context vector)는 sum(어텐션 가중치 * EO, axis = 1)로 계산합니다. 위와 같은 이유로 첫번째 축을 선택합니다.\n",
    "임베딩 결과(embedding output)는 디코더 X에 대한 입력이 임베딩층을 통과한 결과입니다.\n",
    "병합된 벡터(merged vector)는 concat(임베딩 결과, 컨텍스트 백터(context vector))와 같습니다.\n",
    "그런 다음 병합된 벡터는 GRU에 주어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 문제 3.2.1 Encoder & Attention & Decoder 각 Class 이해하고 주석 달기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model): #tf.keras.Model 클래스를 상속받아 Encoder Class 구현\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz): #4가지 파라미터를 이용하는 인코더 생성자 메소드\n",
    "        # 1) vocab_size : 어휘 수(=input 랭귀지에서 unique한 토큰의 개수)\n",
    "        # 2) embedding_dim : 임베딩 공간의 차원 값(각 단어는 해당 차원의 밀집벡터로 표현된다)\n",
    "        # 3) enc_units : 인코더의 GRU 레이어 유닛(뉴런) 개수. (이는 인코더가 학습 및 표현할 수 있는 input 시퀀스의 용량을 결정한다)\n",
    "        # 4) batch_sz : 배치 사이즈(학습 중 동시에 처리되는 시퀀스의 개수)\n",
    "        \n",
    "        super(Encoder, self).__init__() # 상위 클래스의 attribute와 메소드 받기 위해 생성자 호출 \n",
    "        \n",
    "        self.batch_sz = batch_sz # 배치 사이즈 설정\n",
    "        self.enc_units = enc_units # 유닛 개수 설정\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 임베딩 레이어 생성. 임베딩 레이어는\n",
    "        # 토큰화된 input sequence를 고정 길이(embedding_dim)의 밀집 벡터로 바꾸어 준다. vocab_size와 embedding_dim 파라미터 필요\n",
    "        \n",
    "        # RNN 레이어(GRU) 생성 - basic RNN이 아닌 GRU를 사용해 long-range dependency를 학습할 수 있도록 한다\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, #GRU 레이어에서의 유닛 개수\n",
    "                                   return_sequences=True, #GRU 레이어가 각 input sequence에 대한 전체 출력 시퀀스를 반환하도록 함  \n",
    "                                   return_state=True, #GRU 레이어가 출력 뿐 아니라 마지막 상태를 출력하도록 함\n",
    "                                   recurrent_initializer='glorot_uniform') #GRU 레이어의 가중치 초깃값을 Glorot uniform 으로 설정\n",
    "\n",
    "        # GlorotUniform은 [-limit, limit]범위로 정규 분포를 기반으로 샘플 값 생성. limit = sqrt(6/(fan_in+fan_out))\n",
    "        # fan_in은 가중치 텐서에 있는 입력층 유닛의 개수, fan_out은 출력층 유닛의 개수\n",
    "    \n",
    "    def call(self, x, hidden): #인코더 클래스의 call method 정의 - call()은 인코더모델이 호출될 때 릴행된다\n",
    "        # x: input data(토큰화된 입력시퀀스)\n",
    "        # hidden: GRU 레이어의 초기 은닉상태\n",
    "        x = self.embedding(x) #입력 시퀀스를 임베딩 레이어에 통과시켜서 밀집 벡터 표현을 얻는다\n",
    "        output, state = self.gru(x, initial_state = hidden) # 출력 시퀀스(output)와 마지막 은닉상태(state)를 계산한다(return_state=True로 설정했기 때문에 두 가지 값을 반환받음)\n",
    "        return output, state #output과 state을 반환\n",
    "\n",
    "    def initialize_hidden_state(self): # 새로운 batch를 처리하기 전 GRU 레이어의 hidden state를 초기화하는 메소드\n",
    "        return tf.zeros((self.batch_sz, self.enc_units)) #(batch_sz, enc_units)의 shape을 가지는 텐서 0으로 초기화\n",
    "  \n",
    "class BahdanauAttention(tf.keras.layers.Layer): #tf.keras.layers.layer 클래스를 상속받아 \n",
    "    def __init__(self, units): #생성자\n",
    "        #생성자 파라미터 'units': 어텐션 메커니즘에서의 차원수(어텐션 스코어를 계산하기 위한 내부 Dense 레이어의 유닛 개수이다)\n",
    "        \n",
    "        super(BahdanauAttention, self).__init__() #상위 클래스의 생성자 호출\n",
    "        \n",
    "        self.W1 = tf.keras.layers.Dense(units) #units 값 개수만큼의 유닛을 가지는 Dense Layer 생성\n",
    "        # W1 레이어는 attention score를 계산하기 전 쿼리 벡터들이 모두 같은 차원수를 가지도록 변환해주는 역할\n",
    "        self.W2 = tf.keras.layers.Dense(units) #units 값 개수만큼의 유닛을 가지는 Dense Layer 생성\n",
    "        # W2 레이어는 attention score를 계산하기 전 인코더 출력값들이 모두 같은 차원수를 가지도록 변환해주는 역할\n",
    "        self.V = tf.keras.layers.Dense(1) #한 개의 unit을 갖는 Dense Layer 생성. \n",
    "        # V 레이어는 attention score를 계산 후 softmax을 취해 attention weight(어텐션 가중치)을 얻는 역할\n",
    "\n",
    "    def call(self, query, values):\n",
    "        #호출자 파라미터 query: 현재 time step에서의 쿼리 벡터. (seq2seq 모델에서 보통 디코더의 마지막 상태)\n",
    "        #호출자 파라미터 values: value값(인코더에서 모든 time step에서의 output)\n",
    "        \n",
    "        #query 벡터에 time axis를 추가한다. 이는 query의 shape을 values의 shape과 맞추어 주기 위함\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        #어텐션 스코어 계산. 각각 W1,W2를 이용해 query와 values 값을 변환하고 그 둘을 더한 다음, \n",
    "        #hyperbolic tangent 활성화 함수를 적용한 결과를 V에 통과시킨다.\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        #softmax 함수(모든 가중치값의 합 1 되게끔)를 적용해 attention weights 계산. \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        #attention weights와 values를 element-wise multiplication 통해 컨텍스트 벡터 계산.(weighted sum)\n",
    "        context_vector = attention_weights * values\n",
    "\n",
    "        #최종 컨텍스트 벡터를 얻기 위해 시간 축을 기준으로 가중치 값 합산. 이는 어텐션 메커니즘을 기반으로\n",
    "        #입력 시퀀스에서 관련 정보를 캡처하는 벡터이다.\n",
    "        #x = tf.constant([[1,1,1],[1,1,1]])\n",
    "        #tf.reduce_sum(x) -> 6\n",
    "        #tf.reduce_sum(x,axis=0) -> [2,2,2]\n",
    "        #tf.reduce_sum(x,axis=1) -> [3,3]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        #context vector와 attention weights를 반환.\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class Decoder(tf.keras.Model): #tf.keras.Model 클래스를 상속받아 Decoder Class 구현\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        # 1) vocab_size : 어휘 수(target 랭귀지의)\n",
    "        # 2) embedding_dim : 임베딩 공간의 차원 값(target 언어에서의 각 단어는 해당 차원의 밀집벡터로 표현된다)\n",
    "        # 3) dec_units : 디코더의 GRU 게이어 유닛(뉴런) 개수. \n",
    "        # 4) batch_sz : 배치 사이즈 \n",
    "\n",
    "        super(Decoder, self).__init__() #상속받은 상위 클래스의 생성자 호출\n",
    "        \n",
    "        #attribute 값 설정\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # 디코더 임베딩 레이어 생성.토큰화된 input sequence를 고정 길이(embedding_dim)의 밀집 벡터로 바꾸어 준다\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # RNN 레이어(GRU) 생성 - dec_units 개의 유닛을 가지고 return_sequence, return_state를 모두 True로 설정해 줌으로써\n",
    "        # 각 input sequence에 대한 전체 출력 시퀀스, 디코더의 마지막 상태도 받도록.\n",
    "        # GRU 레이어의 가중치 초깃값을 Glorot uniform 으로 설정\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        # Fully connected layer 생성 <- target vocabulary 상 확률 분포를 얻기 위함\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # BahdanauAttention 클래스의 인스턴스를 생성하여 어텐션 메커니즘을 이용해 디코더가 디코딩 과정에서 그때그때 다른\n",
    "        # input sequence의 부분에 더 집중할 수 있도록 한다\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output): #호출자\n",
    "        # x: 현재 time step의 input data(토큰화된 시퀀스)\n",
    "        # hidden: 이전 time step의 GRU 레이어 은닉상태--처음은 인코더에서 마지막 hidden state\n",
    "        # enc_output: input sequence의 정보를 담고 있는 인코더의 출력\n",
    "\n",
    "        # Bahdanau Attention을 통해 context vector와 attention weights를 받는다\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        # input data x를 임베딩 레이어에 통과시켜서 밀집 벡터 표현을 얻는다\n",
    "        x = self.embedding(x)\n",
    "        # 디코더의 GRU의 입력으로 쓰기 위해 context vector와 x를 concatenate\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # x를 GRU 유닛에 통과시켜서 output sequence와 마지막 hidden state를 얻는다\n",
    "        output, state = self.gru(x)\n",
    "        #output 텐서를 reshape. reshape에 -1을 쓰면 다른 shape 값만 제대로 맞추도록 알아서 계산하라는 의미(텐서의 기존 요소들 모두 유지되도록)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        #reshape 된 텐서를 fully connected layer에 통과시켜서 모든 타겟 어휘 상의 확률 분포(output logits)를 얻는다\n",
    "        x = self.fc(output)\n",
    "        # x(output logits), state(final hidden state), attention weights를 반환\n",
    "        #x는 예측된 타겟 시퀀스를 생성하기 위해, state는 다음 time step에 전달되기 위해\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Attention-Decoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7961, 3211)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inp_size, vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 97, 512)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 512)\n",
      "Attention result shape: (batch size, units) (64, 512)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 97, 1)\n",
      "Decoder output shape: (batch_size, vocab size) (64, 3211)\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# 샘플 입력\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "# encoder-decoder attention\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "# decoder\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#Sparse Categorical Crossentropy \n",
    "#from_logits: whether y_pred is expected to be a logits tensor.\n",
    "#ruduction: Type of reduction to apply to the loss. In almost all cases this should be \n",
    "#'sum_over_batch_size'. supported options are {sum/sum over batch size} or None\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # creates a mask where 'True' indicates the positions of non-padding tokens(where 'real' is not equal to 0),\n",
    "    # and 'False' indicates the positions of padding tokens. \n",
    "    loss_ = loss_object(real, pred)\n",
    "    # calculates the loss between the true labels(real) and the predicted labels(pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # converts the mask to the same data type as the computed loss. This step is necessary to \n",
    "    # ensure that the mask can be multiplied element-wise with the loss.\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_) #calculates the mean of the masked loss values\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!\n",
    "\n",
    "언어 모델 훈련하기\n",
    "1. 인코더 결과와 인코더 은닉 상태(hidden state)를 반환하는 인코더를 통해서 입력을 전달합니다.\n",
    "2. 인코더 결과, 인코더 은닉 상태(hidden state), 디코더 입력 (start 토큰)을 디코더에 전달합니다.\n",
    "3. 전달 받은 값을 통해 디코더는 예측 값과 디코더 은닉 상태(hidden state)를 반환합니다.\n",
    "4. 그 다음에 디코더 은닉 상태(hidden state)가 다시 모델에 전달되고 예측 값을 사용하여 손실을 계산합니다.\n",
    "5. 디코더에 대한 다음 입력을 결정하기 위해서 교사 강요(teacher forcing)를 사용합니다.\n",
    "6. 교사 강요(teacher forcing)는 타겟 단어가 디코더에 다음 입력으로 전달하기 위한 기술입니다.\n",
    "7. 마지막 단계는 그레디언트(gradients)를 계산하여 이를 옵티마이저(optimizer)와 역전파(backpropagate)에 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "      # encoder\n",
    "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "      dec_hidden = enc_hidden\n",
    "      \n",
    "      # decoder\n",
    "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "      # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.\n",
    "      for t in range(1, targ.shape[1]):\n",
    "          # enc_output를 디코더에 전달합니다.\n",
    "          predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "          loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "          # 교사 강요(teacher forcing)를 사용합니다.\n",
    "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  #trainable_variables는 Encoder와 Decoder가 상속받은 클래스에 있는 속성\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / Batch 0 / Loss 0.5508 / Time taken 31.897533655166626 sec\n",
      "Epoch 1 / Batch 10 / Loss 0.3547 / Time taken 342.49861764907837 sec\n",
      "Epoch 1 / Batch 20 / Loss 0.3687 / Time taken 653.7815310955048 sec\n",
      "Epoch 1 / Batch 30 / Loss 0.3527 / Time taken 963.0116488933563 sec\n",
      "Epoch 1 / Batch 40 / Loss 0.3608 / Time taken 1270.88419008255 sec\n",
      "Epoch 1 / Batch 50 / Loss 0.3544 / Time taken 1579.9503571987152 sec\n",
      "Epoch 1 / Batch 60 / Loss 0.3400 / Time taken 1885.4591567516327 sec\n",
      "Epoch 1 / Batch 70 / Loss 0.3801 / Time taken 2191.03932595253 sec\n",
      "Epoch 1 / Loss 0.3738\n",
      "Time taken for 1 epoch 2251.5741336345673 sec\n",
      "\n",
      "Epoch 2 / Batch 0 / Loss 0.3491 / Time taken 29.118820905685425 sec\n",
      "Epoch 2 / Batch 10 / Loss 0.3296 / Time taken 321.31594371795654 sec\n",
      "Epoch 2 / Batch 20 / Loss 0.3057 / Time taken 613.8220045566559 sec\n",
      "Epoch 2 / Batch 30 / Loss 0.3029 / Time taken 910.4554595947266 sec\n",
      "Epoch 2 / Batch 40 / Loss 0.3325 / Time taken 1200.2888412475586 sec\n",
      "Epoch 2 / Batch 50 / Loss 0.3455 / Time taken 1492.1392464637756 sec\n",
      "Epoch 2 / Batch 60 / Loss 0.2995 / Time taken 1871.080243587494 sec\n",
      "Epoch 2 / Batch 70 / Loss 0.2872 / Time taken 2173.143309354782 sec\n",
      "Epoch 2 / Loss 0.3202\n",
      "Time taken for 2 epoch 2239.8778355121613 sec\n",
      "\n",
      "Epoch 3 / Batch 0 / Loss 0.3098 / Time taken 37.95486378669739 sec\n",
      "Epoch 3 / Batch 10 / Loss 0.3248 / Time taken 377.2501974105835 sec\n",
      "Epoch 3 / Batch 20 / Loss 0.3008 / Time taken 694.8423750400543 sec\n",
      "Epoch 3 / Batch 30 / Loss 0.2700 / Time taken 1017.2994403839111 sec\n",
      "Epoch 3 / Batch 40 / Loss 0.2947 / Time taken 1330.7567796707153 sec\n",
      "Epoch 3 / Batch 50 / Loss 0.2887 / Time taken 1658.5948266983032 sec\n",
      "Epoch 3 / Batch 60 / Loss 0.3331 / Time taken 1971.4780609607697 sec\n",
      "Epoch 3 / Batch 70 / Loss 0.2638 / Time taken 2318.9419865608215 sec\n",
      "Epoch 3 / Loss 0.2991\n",
      "Time taken for 3 epoch 2388.8618512153625 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이 파트가 매우 오래걸림\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "      batch_loss = train_step(inp, targ, enc_hidden)\n",
    "      total_loss += batch_loss\n",
    "\n",
    "      if batch % 10 == 0:\n",
    "          print('Epoch {} / Batch {} / Loss {:.4f} / Time taken {} sec'.format(epoch + 1,\n",
    "                                                                                batch,\n",
    "                                                                                batch_loss.numpy(),\n",
    "                                                                                time.time() - start))\n",
    "  # 에포크가 2번 실행될때마다 모델 저장 (체크포인트)\n",
    "  #if (epoch + 1) % 2 == 0:\n",
    "  #    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} / Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "  print('Time taken for {} epoch {} sec\\n'.format(epoch + 1,\n",
    "                                                  time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # 나중에 어텐션 가중치를 시각화하기 위해 어텐션 가중치를 저장합니다.\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # 예측된 ID를 모델에 다시 피드합니다.\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    #print('Input: %s' % (sentence))\n",
    "    #print('Predicted translation: {}'.format(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> 톰이 마실 수 있는 것은 오직 물 뿐이야 <end>\n",
      "Predicted translation: i don't don't be a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a \n"
     ]
    }
   ],
   "source": [
    "translate('톰이 마실 수 있는 것은 오직 물 뿐이야')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 문제 3.2.2 - BLEU 스코어 구현하기\n",
    "#### - Dataset Sample에서 Test 만들기\n",
    "#### - BLEU Score 구현 \n",
    "#### - 학습된 모델로 BLEU Score로 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 input 영어 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU score \n",
    "import collections\n",
    "import math\n",
    "\n",
    "def ngram_counts(sentence, n):\n",
    "  words = sentence.split()\n",
    "  ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "  return collections.Counter(ngrams)\n",
    "  #ngram 시퀀스마다 갯수를 센 Counter 딕셔너리 반환\n",
    "\n",
    "#문장길이에 대한 과적합 보정\n",
    "def brevity_penalty(pred, real):\n",
    "  p_len = len(pred.split())\n",
    "  r_len = len(real.split())\n",
    "  if p_len > r_len:\n",
    "    return 1\n",
    "  else:\n",
    "    return (p_len/r_len)\n",
    "    #return math.exp(1-r_len/p_len) #min(1, pred len/real len)\n",
    "\n",
    "def bleu_score(pred, real, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "  bp = brevity_penalty(pred, real)\n",
    "  ng = 1\n",
    "  \n",
    "  for i, weight in enumerate(weights, start=1):\n",
    "    #i-gram 1부터 4까지\n",
    "    gram_len = len(pred)-i+1 #분모(예측된 문장에서 n-gram 시퀀스의 길이)\n",
    "    cnt = 0\n",
    "    ngram_cnts_pred = ngram_counts(pred, i)\n",
    "    ngram_cnts_real = ngram_counts(real, i)\n",
    "    for key,value in ngram_cnts_pred.items():\n",
    "      if key in ngram_cnts_real:\n",
    "        # cnt += min(value, ngram_cnts_real[key]) \n",
    "        if value > ngram_cnts_real[key]:\n",
    "          cnt += ngram_cnts_real[key] #Clipping\n",
    "        else:\n",
    "          cnt += value\n",
    "    ng *= (cnt / gram_len)**weight\n",
    "  \n",
    "  return bp * ng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data로 성능 평가해보기(BLUE)\n",
    "bleu = 0\n",
    "for i, sample in enumerate(input_tensor_val):\n",
    "  \n",
    "  inp_sent = ''\n",
    "  for w in sample:\n",
    "    if w!=0:\n",
    "      kw = inp_lang.index_word[w]\n",
    "      if kw != '<start>' and kw != '<end>':\n",
    "        inp_sent = inp_sent + kw +' '\n",
    "  #print(inp_sent)\n",
    "\n",
    "  pred_sent = translate(inp_sent).replace(' <end> ','')\n",
    "  \n",
    "  targ_sent = ''\n",
    "  for w in target_tensor_val[i]:\n",
    "    if w!=0:\n",
    "      ew = targ_lang.index_word[w]\n",
    "      if ew != '<start>' and ew != '<end>':\n",
    "        targ_sent = targ_sent + ew + ' '\n",
    "  #print(targ_sent)\n",
    "  bleu += bleu_score(pred_sent, targ_sent)\n",
    "  \n",
    "  if i%100 == 0:\n",
    "    print(f'{i+1}samples sampled')\n",
    "  \n",
    "print(f\"총합산한 BLEU스코어: {bleu:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{bleu:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{bleu*200:.60f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
