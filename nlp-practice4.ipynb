{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":80316,"databundleVersionId":8589663,"sourceType":"competition"},{"sourceId":59907,"sourceType":"modelInstanceVersion","modelInstanceId":50108}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Practice 4 - Question & Answering with BERT","metadata":{}},{"cell_type":"markdown","source":"### 실습 4.1 - Load SQuAD Raw Data with JSON\n### SQuAD 데이터 살펴보기","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\nfilename = \"/kaggle/input/2024-1-nlp-4/train-v2.json\"\n\nwith open(filename, \"r\", encoding='utf-8') as reader:\n    input_data = json.load(reader)[\"data\"]\n    \nfor entry in input_data:\n    for paragraph in entry[\"paragraphs\"]:\n        context = paragraph['context']\n        print(context)\n        print()\n        \n        for qa in paragraph['qas']:\n            is_impossible = qa['is_impossible']\n\n            if not is_impossible:\n                answer = qa['answers'][0]\n                original_answer = answer['text']\n                answer_start = answer['answer_start']\n                \n            qid=qa['id'],\n            question=qa['question'],\n                \n            print(qid, question, answer)\n        \n    \n        break\n    break","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:19:29.845776Z","iopub.execute_input":"2024-06-03T03:19:29.846735Z","iopub.status.idle":"2024-06-03T03:19:31.215014Z","shell.execute_reply.started":"2024-06-03T03:19:29.846707Z","shell.execute_reply":"2024-06-03T03:19:31.213973Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n\n('56be85543aeaaa14008c9063',) ('When did Beyonce start becoming popular?',) {'text': 'in the late 1990s', 'answer_start': 269}\n('56be85543aeaaa14008c9065',) ('What areas did Beyonce compete in when she was growing up?',) {'text': 'singing and dancing', 'answer_start': 207}\n('56be85543aeaaa14008c9066',) (\"When did Beyonce leave Destiny's Child and become a solo singer?\",) {'text': '2003', 'answer_start': 526}\n('56bf6b0f3aeaaa14008c9601',) ('In what city and state did Beyonce  grow up? ',) {'text': 'Houston, Texas', 'answer_start': 166}\n('56bf6b0f3aeaaa14008c9602',) ('In which decade did Beyonce become famous?',) {'text': 'late 1990s', 'answer_start': 276}\n('56bf6b0f3aeaaa14008c9603',) ('In what R&B group was she the lead singer?',) {'text': \"Destiny's Child\", 'answer_start': 320}\n('56bf6b0f3aeaaa14008c9604',) ('What album made her a worldwide known artist?',) {'text': 'Dangerously in Love', 'answer_start': 505}\n('56bf6b0f3aeaaa14008c9605',) (\"Who managed the Destiny's Child group?\",) {'text': 'Mathew Knowles', 'answer_start': 360}\n('56d43c5f2ccc5a1400d830a9',) ('When did Beyoncé rise to fame?',) {'text': 'late 1990s', 'answer_start': 276}\n('56d43c5f2ccc5a1400d830aa',) (\"What role did Beyoncé have in Destiny's Child?\",) {'text': 'lead singer', 'answer_start': 290}\n('56d43c5f2ccc5a1400d830ab',) ('What was the first album Beyoncé released as a solo artist?',) {'text': 'Dangerously in Love', 'answer_start': 505}\n('56d43c5f2ccc5a1400d830ac',) ('When did Beyoncé release Dangerously in Love?',) {'text': '2003', 'answer_start': 526}\n('56d43c5f2ccc5a1400d830ad',) ('How many Grammy awards did Beyoncé win for her first solo album?',) {'text': 'five', 'answer_start': 590}\n('56d43ce42ccc5a1400d830b4',) (\"What was Beyoncé's role in Destiny's Child?\",) {'text': 'lead singer', 'answer_start': 290}\n('56d43ce42ccc5a1400d830b5',) (\"What was the name of Beyoncé's first solo album?\",) {'text': 'Dangerously in Love', 'answer_start': 505}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 실습 4.2 - SQuAD Dataset Class 생성 (from raw data to tokenized version)","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom torch.utils.data import Dataset, TensorDataset\n\n# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"/kaggle/input/2024-1-nlp-4/feature.py\", dst = \"/kaggle/working/feature.py\")\nfrom feature import convert_examples_to_features\n\ndef is_whitespace(c):\n    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n        return True\n    return False\n\nclass SquadExample():\n    def __init__(self, qid, context, question, answer, start, end, is_impossible):\n        self.qid = qid\n        self.context = context\n        self.question = question\n        self.answer = answer\n        self.start = start\n        self.end = end\n        self.is_impossible = is_impossible\n        \n    def __repr__(self):\n        #return self.context[self.start:self.end]\n        #if self.context[self.start:self.end] != self.answer:\n        #    return 'NA!! {} - {}'.format(self.context[self.start:self.end], answer)\n        return 'id:{}  question:{}...  answer:{}...  is_impossible:{}'.format(\n            self.qid,\n            self.question[:10],\n            self.answer[:10],\n            self.is_impossible)\n\nclass SquadDataset(Dataset):\n    def __init__(self, path, tokenizer, is_train=True, is_inference=False):\n        '''\n        path: SquadDataset 데이터셋 위치\n        tokenizer: Squad 데이터셋을 토크나이징할 토크나이저, ex) BertTokenizer\n        is_train: SquadDataset을 정의하는 목적이 모델 학습용일 경우 True, 그렇지 않으면 False\n        is_inference: SquadDataset을 정의하는 목적이 인퍼런스용일 경우 True, 그렇지 않으면 False\n        '''\n        \n        if is_train:\n            filename = os.path.join(path, 'train-v2.json')\n        else:\n            if is_inference:\n                filename = os.path.join(path, 'test-v2.json')\n            else:\n                filename = os.path.join(path, 'dev-v2.json')\n\n        cached_features_file = os.path.join(os.path.dirname(filename), 'cached_{}_64.cache'.format('train' if is_train else 'valid'))\n        #cached_examples_file = os.path.join(os.path.dirname(filename), 'cached_example_{}_64.cache'.format('train' if is_train else 'valid'))\n\n        if os.path.exists(cached_features_file):\n            print('cache file exists')\n            self.features = torch.load(cached_features_file)\n        else:\n            print('cache file does not exist')\n\n            with open(filename, \"r\", encoding='utf-8') as reader:\n                input_data = json.load(reader)[\"data\"]\n\n            self.examples = []\n            number_of_examples = 100\n            for entry in input_data[:number_of_examples]:\n                for paragraph in entry[\"paragraphs\"]:\n                    context = paragraph['context']\n                    \n                    doc_tokens = []\n                    char_to_word_offset = []\n                    prev_is_whitespace = True\n                    for c in context:\n                        if is_whitespace(c):\n                            prev_is_whitespace = True\n                        else:\n                            if prev_is_whitespace:\n                                doc_tokens.append(c)\n                            else:\n                                doc_tokens[-1] += c\n                            prev_is_whitespace = False\n                        char_to_word_offset.append(len(doc_tokens) - 1)\n                            \n                            \n                    for qa in paragraph['qas']:\n                        is_impossible = qa['is_impossible']\n                        \n                        if not is_impossible:\n                            answer = qa['answers'][0]\n                            original_answer = answer['text']\n                            answer_start = answer['answer_start']\n                            \n                            answer_length = len(original_answer)\n                            start_pos = char_to_word_offset[answer_start]\n                            end_pos = char_to_word_offset[answer_start + answer_length - 1]\n\n                            answer_end = answer_start + len(original_answer)\n                        else:\n                            original_answer = ''\n                            start_pos = 1\n                            end_pos = -1\n\n                        example = SquadExample(\n                            qid=qa['id'],\n                            context=doc_tokens,\n                            question=qa['question'],\n                            answer=original_answer,\n                            start=start_pos,\n                            end=end_pos,\n                            is_impossible=is_impossible)\n                        self.examples.append(example)\n            print('examples: {}'.format(len(self.examples)))\n\n            self.features = convert_examples_to_features(\n                examples=self.examples,\n                tokenizer=tokenizer,\n                max_seq_length=384,\n                doc_stride=128,\n                max_query_length=64,\n                is_training=True if not is_inference else False)\n            print('is_training: {}'.format(True if not is_inference else False))\n\n            # torch.save(self.examples, cached_examples_file)\n            # torch.save(self.features, cached_features_file)\n\n        '''\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in self.features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in self.features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in self.features], dtype=torch.long)\n        all_cls_index = torch.tensor([f.cls_index for f in self.features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in self.features], dtype=torch.float)\n        if is_train:\n            all_start_positions = torch.tensor([f.start_position for f in self.features], dtype=torch.long)\n            all_end_positions = torch.tensor([f.end_position for f in self.features], dtype=torch.long)\n            dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n                                    all_start_positions, all_end_positions,\n                                    all_cls_index, all_p_mask)\n        else:\n            all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n            dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index, all_cls_index, all_p_mask)\n        return dataset\n        '''\n\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-03T03:19:39.650692Z","iopub.execute_input":"2024-06-03T03:19:39.651050Z","iopub.status.idle":"2024-06-03T03:19:42.916245Z","shell.execute_reply.started":"2024-06-03T03:19:39.651020Z","shell.execute_reply":"2024-06-03T03:19:42.915226Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 실습 4.3 - SQuAD DataLoader 생성 (from raw data to tokenized version)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\nclass SquadDataLoader(DataLoader):\n    def __init__(self, dataset, batch_size, is_inference=False, shuffle=True):\n        '''\n        dataset: SquadDataset으로 정의한 데이터셋 객체\n        batch_size: 배치 사이즈\n        is_inference: SquadDataLoader를 인퍼런스 목적으로 사용할 경우 True, 그렇지 않으면 False\n        shuffle: 데이터의 순서를 섞을 경우 True, 그렇지 않으면 False\n        '''\n        self.is_inference = is_inference\n        super().__init__(dataset, collate_fn=self.squad_collate_fn, batch_size=batch_size, shuffle=shuffle)\n        \n    def squad_collate_fn(self, features):\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n\n        # return 6 tensors\n        if self.is_inference:\n            all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n            return all_input_ids, all_input_mask, all_segment_ids, all_cls_index, all_p_mask, all_example_index\n        # return 7 tensors\n        else:\n            all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n            all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n            return all_input_ids, all_input_mask, all_segment_ids, all_cls_index, all_p_mask, all_start_positions, all_end_positions","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:20:09.271468Z","iopub.execute_input":"2024-06-03T03:20:09.271936Z","iopub.status.idle":"2024-06-03T03:20:09.284161Z","shell.execute_reply.started":"2024-06-03T03:20:09.271906Z","shell.execute_reply":"2024-06-03T03:20:09.283268Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 실습 4.4 Load Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm, trange\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer\n\npath = \"/kaggle/input/2024-1-nlp-4/\"\n\nprint(\"Tokenizer Loading\")\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n#print(\"Dataset Loading\")\n#train_dataset = SquadDataset(path, tokenizer, is_train=True) # 153,000\n\n#print(\"Data Loader\")\n#train_dataloader = SquadDataLoader(train_dataset, batch_size=32, is_inference=False, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:20:15.292157Z","iopub.execute_input":"2024-06-03T03:20:15.292519Z","iopub.status.idle":"2024-06-03T03:20:17.693996Z","shell.execute_reply.started":"2024-06-03T03:20:15.292487Z","shell.execute_reply":"2024-06-03T03:20:17.693058Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Tokenizer Loading\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28537bfe3bad4d0b8162195c7b6d7757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba2db67232d348b2a5d169123324df31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d7533f7d8f496e8ef7c1f896dcac6b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e960527771c4817beb83adf64a019c7"}},"metadata":{}}]},{"cell_type":"markdown","source":"### 실습 4.5 - Load Pre-trained BERT\n### 과제 4.1 - BERT for Question Answering 모델 이해하고 설명하기 / Tokenizer 변경해보기\n\n#### BERT for Question Answering 참고\n#### https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/bert#transformers.BertForQuestionAnswering\n\n#### BERT Tokenizer 참고\n#### https://huggingface.co/docs/transformers/v4.41.0/en/model_doc/bert#transformers.BertTokenizer","metadata":{}},{"cell_type":"code","source":"# pytoch model import from huggingface\nfrom transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n\n# GPU 이용 방법 - Notebook Option - Session Options - ACCELRATOR 설정 (GPU P100)\n# .cuda() 옵션을 제거하면 cpu에서도 학습 가능\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased').cuda()\n\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:52:37.774158Z","iopub.execute_input":"2024-06-02T16:52:37.775074Z","iopub.status.idle":"2024-06-02T16:52:41.831191Z","shell.execute_reply.started":"2024-06-02T16:52:37.775030Z","shell.execute_reply":"2024-06-02T16:52:41.830307Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d944d7eb1e4cbe9fe32f0073a3439c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### 실습 4.6 - Fine-tuning with your SQuAD Dataset","metadata":{}},{"cell_type":"code","source":"# train fucntion\ndef train(model, dataloader, optimizer):\n    tbar = tqdm(dataloader, desc='Training', leave=True)\n    \n    total_loss = 0.0\n    for i, batch in enumerate(tbar):\n        optimizer.zero_grad()\n        \n        # cls_index와 p_mask는 XLNet 모델에 사용되므로 BERT에서는 사용하지 않는다.\n        input_ids, input_mask, segment_ids, cls_index, p_mask, start_positions, end_positions = batch\n        \n        # to cuda (gpu 사용 시)\n        input_ids = input_ids.cuda()\n        input_mask = input_mask.cuda()\n        segment_ids = segment_ids.cuda()\n        start_positions = start_positions.cuda()\n        end_positions = end_positions.cuda()\n        \n        # train model\n        #out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        inputs = {\n            'input_ids': input_ids,\n            'token_type_ids': segment_ids,\n            'attention_mask': input_mask,\n        }\n        out = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n        loss = out.loss\n\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.data.item()\n        tbar.set_description(\"Average Loss = {:.4f})\".format(total_loss/(i+1)))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:57:18.543236Z","iopub.execute_input":"2024-06-02T09:57:18.543637Z","iopub.status.idle":"2024-06-02T09:57:18.552294Z","shell.execute_reply.started":"2024-06-02T09:57:18.543607Z","shell.execute_reply":"2024-06-02T09:57:18.551270Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTrain (Fine-tune) your BERT with SQuAD dataset\n\"\"\"\n\noptimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\nloss = nn.CrossEntropyLoss()\nn_epoch = 3\n\n# actual training\nfor i in range(n_epoch):\n    train(model, train_dataloader, optimizer)\n\n\n# save model\n# torch.save(model.state_dict(), 'squad_model.bin')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:57:33.475889Z","iopub.execute_input":"2024-06-02T09:57:33.476913Z","iopub.status.idle":"2024-06-02T10:54:43.658356Z","shell.execute_reply.started":"2024-06-02T09:57:33.476850Z","shell.execute_reply":"2024-06-02T10:54:43.657442Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nAverage Loss = 1.7845): 100%|██████████| 937/937 [19:02<00:00,  1.22s/it]\nAverage Loss = 0.9764): 100%|██████████| 937/937 [19:03<00:00,  1.22s/it]\nAverage Loss = 0.6722): 100%|██████████| 937/937 [19:04<00:00,  1.22s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'squad_model.bin')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T10:58:22.849021Z","iopub.execute_input":"2024-06-02T10:58:22.849386Z","iopub.status.idle":"2024-06-02T10:58:23.470737Z","shell.execute_reply.started":"2024-06-02T10:58:22.849357Z","shell.execute_reply":"2024-06-02T10:58:23.469702Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 과제 4.2 Inference 및 Evaluate\n\n- 파인튜닝을 마치고 dev-v2.json 파일을 불러와 Inference를 위한 코드를 실행한다.\n- 예측한 span과 정답 span을 비교해본다.\n- F1을 이용하여 dev-v2.json의 샘플 1000개를 대상으로 예측한 span과 정답 span을 평가하는 코드를 작성한다.\n\n아래 평가용 코드 참고\n\n- https://github.com/jinkilee/hello-transformer/blob/master/research/chapter4/squad/run_evaluate.py\n- https://github.com/jinkilee/hello-transformer/blob/master/research/chapter4/squad/evaluate.py","metadata":{}},{"cell_type":"code","source":"valid_dataset = SquadDataset(path, tokenizer, is_train=False) # 11,873\nvalid_dataloader = SquadDataLoader(valid_dataset, batch_size=32, is_inference=False, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:20:56.214934Z","iopub.execute_input":"2024-06-03T03:20:56.215304Z","iopub.status.idle":"2024-06-03T03:23:15.920836Z","shell.execute_reply.started":"2024-06-03T03:20:56.215276Z","shell.execute_reply":"2024-06-03T03:23:15.919959Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"cache file does not exist\nexamples: 11873\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 11873/11873 [02:18<00:00, 85.55it/s] ","output_type":"stream"},{"name":"stdout","text":"is_training: True\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForQuestionAnswering\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased').cuda()\nmodel.load_state_dict(torch.load('/kaggle/input/squad_model/pytorch/squad/1/squad_model.bin'))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:23:40.897667Z","iopub.execute_input":"2024-06-03T03:23:40.898039Z","iopub.status.idle":"2024-06-03T03:23:48.486508Z","shell.execute_reply.started":"2024-06-03T03:23:40.898007Z","shell.execute_reply":"2024-06-03T03:23:48.485392Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a7e4cb7d62c4b2994ce9d2b8731606d"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\")\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:24:29.555658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 학습된 모델이 예측한 결과와 주어진 validation 데이터셋과 비교해본다.\nimport pandas as pd\ndef inference(model, tokenizer):\n  answer_sheet = []\n  num_batches = 3\n  for i in range(num_batches):\n    all_input_ids, all_input_mask, all_segment_ids, all_cls_index, all_p_mask, all_start_positions, all_end_positions = next(iter(valid_dataloader))\n    for j in range(batch_size):\n        input_ids = all_input_ids[j].unsqueeze(0).to(device)\n        token_type_ids = all_segment_ids[j].unsqueeze(0).to(device)\n        attention_mask = all_input_mask[j].unsqueeze(0).to(device)\n        output = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n        predict_start_index = output.start_logits.argmax()\n        predict_end_index = output.end_logits.argmax()\n        predict_answer_tokens = all_input_ids[j][predict_start_index:predict_end_index+1]\n        pred = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n        \n        answ = tokenizer.decode(all_input_ids[j][all_start_positions[j]:all_end_positions[j]+1])\n    \n        answer_sheet.append([answ, pred])\n  \n  df = pd.DataFrame(answer_sheet, columns=['Answer', 'Prediction'])\n  df.to_csv('/kaggle/working/answer.csv', index=False)\n  print(df)\n  return\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:24:07.597153Z","iopub.execute_input":"2024-06-03T03:24:07.597522Z","iopub.status.idle":"2024-06-03T03:24:07.914513Z","shell.execute_reply.started":"2024-06-03T03:24:07.597484Z","shell.execute_reply":"2024-06-03T03:24:07.913736Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"inference(model, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine-tuned된 데이터셋을 평가한다.\n\ndef evaluate(model, tokenizer):\n\"\"\"\nWrite your code here\n\"\"\"\n\n    \ndef main():\n    # 모델 정의\n    model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\", num_labels = 2).to(device)\n    #model.load_state_dict(torch.load('models/squad_model.bin'))\n    model.eval()\n\n    model.to(args.device)\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n    evaluate(model, tokenizer)","metadata":{},"execution_count":null,"outputs":[]}]}