{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1GbfNYmgEYfHQcfxfib2zzaoYKYFN1khH","authorship_tag":"ABX9TyObGKIPrB+AJM6vBkx5QJnF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0GHxL-lZyAe","executionInfo":{"status":"ok","timestamp":1714973918552,"user_tz":-540,"elapsed":71444,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"ad291e6f-3f0a-4d00-d350-02460fe256be"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import numpy as np\n","import os\n","import io\n","import re\n","import time"],"metadata":{"id":"2dGiwnCzah--","executionInfo":{"status":"ok","timestamp":1714974241199,"user_tz":-540,"elapsed":8921,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s) # decompose string 's' into its base characters and combining characters using the Normalization Form D(NFD) scheme\n","                 if unicodedata.category(c)!= 'Mn') #checks if the character's category, obtained using unicodedata.category(), is not equal to 'Mn'\n","#(Mn stands for 'Mark, Nonspacing' which includes diacritic marks like accents, umlauts, etc. So this condition effecively filters out all diacritic marks from the string)\n","\n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","  # 단어와 단어 뒤에 오는 구두점(.)사이에 공백을 생성.\n","    # 예시: \"he is a boy.\" => \"he is a boy .\"\n","    # 참고:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \",w)\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")을 제외한 모든 것을 공백으로 대체합니다.\n","  # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","  w = w.strip()\n","\n","  # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해서\n","    # 문장에 start와 end 토큰을 추가합니다.\n","\n","  w = '<start> ' + w + ' <end>'\n","  return w"],"metadata":{"id":"VUte1R-Mah85","executionInfo":{"status":"ok","timestamp":1714974243642,"user_tz":-540,"elapsed":3,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# 1. 문장에 있는 억양을 제거합니다.\n","# 2. 불필요한 문자를 제거하여 문장을 정리합니다.\n","# 3. 다음과 같은 형식으로 문장의 쌍을 반환합니다: [영어, 스페인어]\n","\n","def create_dataset(path, num_examples):\n","  ens = []\n","  spas = []\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","  for l in lines[:num_examples]:\n","    word_pairs = [preprocess_sentence(w) for w in l.split('\\t')[:2]]\n","    en, spa = word_pairs\n","    ens.append(en)\n","    spas.append(spa)\n","  return ens, spas\n","\n","path_to_file = \"/content/drive/MyDrive/dataset/kor.txt\"\n","en, kor = create_dataset(path_to_file, None)\n","\n","print(en[-1])\n","print(kor[-1])\n","\n","print(len(en))\n","print(len(kor))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5tSZlrgah6o","executionInfo":{"status":"ok","timestamp":1714974246795,"user_tz":-540,"elapsed":1252,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"f5765b49-b5f1-4432-df7c-e010a2bd808a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n","<start> 의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반대의 상황이 존재하지 . 그런데 인간이 수백 명의 사람만 알고 지내는 사이가 될 기회를 갖는다고 생각해 보면 , 또 그 수백 명 중 열여 명 쯤 이하만 잘 알 수 있고 , 그리고 나서 그 열여 명 중에 한두 명만 친구가 될 수 있다면 , 그리고 또 만일 우리가 이 세상에 살고 있는 수백만 명의 사람들만 기억하고 있다면 , 딱 맞는 남자는 지구가 생겨난 이래로 딱 맞는 여자를 단 한번도 만난 적이 없을 수도 있을 거라는 사실을 쉽게 눈치챌 수 있을 거야 . <end>\n","5890\n","5890\n"]}]},{"cell_type":"code","source":["def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') #no characters are filtered out during tokenization\n","  lang_tokenizer.fit_on_texts(lang) #fits the tokenizer on the input text 'lang'. This step updates the internal vocabulary of the tokenizer based on the words(or tokens) present in the input text\n","  tensor = lang_tokenizer.texts_to_sequences(lang) #converts the text sequences in 'lang' into sequence of integers.(index according to the vocabulary learned by the previous step)\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # pads the sequences of integers in 'tensor' to ensure that they all have the same length\n","  return tensor, lang_tokenizer"],"metadata":{"id":"JTrktIRiah4B","executionInfo":{"status":"ok","timestamp":1714974248822,"user_tz":-540,"elapsed":3,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def load_dataset(path, num_examples=None):\n","  # 전처리된 타겟 문장과 입력 문장 쌍을 생성합니다.\n","  targ_lang, inp_lang = create_dataset(path, num_examples)\n","  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"metadata":{"id":"8e08ga-yah1a","executionInfo":{"status":"ok","timestamp":1714974250406,"user_tz":-540,"elapsed":4,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 언어 데이터셋을 아래의 크기로 제한하여 훈련과 검증을 수행합니다.\n","num_examples = 10000\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n","\n","# 타겟 텐서와 입력 텐서의 최대 길이를 계산합니다.\n","max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n","\n","# 훈련 집합과 검증 집합을 80대 20으로 분리합니다.\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","\n","# 훈련 집합과 검증 집합의 데이터 크기를 출력합니다.\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OlaiBXMpahyv","executionInfo":{"status":"ok","timestamp":1714974252872,"user_tz":-540,"elapsed":609,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"36ff9c1d-0864-43f6-c5c2-5ee827b9646d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["4712 4712 1178 1178\n"]}]},{"cell_type":"code","source":["print(max_length_targ, max_length_inp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PaVDNQHOahwP","executionInfo":{"status":"ok","timestamp":1714974254648,"user_tz":-540,"elapsed":6,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"41cd73b9-dd83-4af8-f729-6046a8f4e216"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["112 97\n"]}]},{"cell_type":"code","source":["def convert(lang, tensor):\n","    for t in tensor:\n","        if t!=0:\n","            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n","\n","print (\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4Absxgwahtz","executionInfo":{"status":"ok","timestamp":1714974256531,"user_tz":-540,"elapsed":6,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"d1f29ad0-4566-45dd-86c7-a8c109b87d41"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Language; index to word mapping\n","1 ----> <start>\n","6100 ----> 봤지만\n","227 ----> 아무것도\n","32 ----> 안\n","2334 ----> 보였다\n","3 ----> .\n","2 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","4 ----> i\n","407 ----> looked\n","15 ----> ,\n","106 ----> but\n","4 ----> i\n","53 ----> didn't\n","142 ----> see\n","156 ----> anything\n","3 ----> .\n","2 ----> <end>\n"]}]},{"cell_type":"code","source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","# calculates the number of steps per epoch based on the length of the training input tensor and the chosen batch size.\n","# for defining the number of iterations per epoch during training\n","embedding_dim = 128\n","units = 512 # number of units in the recurrent units\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","#+1 is for accomodating the padding token.\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) #drop_remainder: ensures that any remaining samples that don't fit into a full batch are dropped."],"metadata":{"id":"KErQATQdahq9","executionInfo":{"status":"ok","timestamp":1714974260529,"user_tz":-540,"elapsed":630,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#dataset 크기출력 ///왜 1차원???not 2차원?? dataset 오타.\n","example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0Y76Jjdahon","executionInfo":{"status":"ok","timestamp":1714974262029,"user_tz":-540,"elapsed":6,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"fa3ba82e-8872-4db6-a506-8375f1993a07"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 97]), TensorShape([64, 112]))"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["class Encoder(tf.keras.Model): #tf.keras.Model 클래스를 상속받아 Encoder Class 구현\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz): #4가지 파라미터를 이용하는 인코더 생성자 메소드\n","        # 1) vocab_size : 어휘 수(=input 랭귀지에서 unique한 토큰의 개수)\n","        # 2) embedding_dim : 임베딩 공간의 차원 값(각 단어는 해당 차원의 밀집벡터로 표현된다)\n","        # 3) enc_units : 인코더의 GRU 레이어 유닛(뉴런) 개수. (이는 인코더가 학습 및 표현할 수 있는 input 시퀀스의 용량을 결정한다)\n","        # 4) batch_sz : 배치 사이즈(학습 중 동시에 처리되는 시퀀스의 개수)\n","\n","        super(Encoder, self).__init__() # 상위 클래스의 attribute와 메소드 받기 위해 생성자 호출\n","\n","        self.batch_sz = batch_sz # 배치 사이즈 설정\n","        self.enc_units = enc_units # 유닛 개수 설정\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 임베딩 레이어 생성. 임베딩 레이어는\n","        # 토큰화된 input sequence를 고정 길이(embedding_dim)의 밀집 벡터로 바꾸어 준다. vocab_size와 embedding_dim 파라미터 필요\n","\n","        # RNN 레이어(GRU) 생성 - basic RNN이 아닌 GRU를 사용해 long-range dependency를 학습할 수 있도록 한다\n","        self.gru = tf.keras.layers.GRU(self.enc_units, #GRU 레이어에서의 유닛 개수\n","                                   return_sequences=True, #GRU 레이어가 각 input sequence에 대한 전체 출력 시퀀스를 반환하도록 함\n","                                   return_state=True, #GRU 레이어가 출력 뿐 아니라 마지막 상태를 출력하도록 함\n","                                   recurrent_initializer='glorot_uniform') #GRU 레이어의 가중치 초깃값을 Glorot uniform 으로 설정\n","\n","        # GlorotUniform은 [-limit, limit]범위로 정규 분포를 기반으로 샘플 값 생성. limit = sqrt(6/(fan_in+fan_out))\n","        # fan_in은 가중치 텐서에 있는 입력층 유닛의 개수, fan_out은 출력층 유닛의 개수\n","\n","    def call(self, x, hidden): #인코더 클래스의 call method 정의 - call()은 인코더모델이 호출될 때 릴행된다\n","        # x: input data(토큰화된 입력시퀀스)\n","        # hidden: GRU 레이어의 초기 은닉상태\n","        x = self.embedding(x) #입력 시퀀스를 임베딩 레이어에 통과시켜서 밀집 벡터 표현을 얻는다\n","        output, state = self.gru(x, initial_state = hidden) # 출력 시퀀스(output)와 마지막 은닉상태(state)를 계산한다(return_state=True로 설정했기 때문에 두 가지 값을 반환받음)\n","        return output, state #output과 state을 반환\n","\n","    def initialize_hidden_state(self): # 새로운 batch를 처리하기 전 GRU 레이어의 hidden state를 초기화하는 메소드\n","        return tf.zeros((self.batch_sz, self.enc_units)) #(batch_sz, enc_units)의 shape을 가지는 텐서 0으로 초기화\n","\n","class BahdanauAttention(tf.keras.layers.Layer): #tf.keras.layers.layer 클래스를 상속받아\n","    def __init__(self, units): #생성자\n","        #생성자 파라미터 'units': 어텐션 메커니즘에서의 차원수(어텐션 스코어를 계산하기 위한 내부 Dense 레이어의 유닛 개수이다)\n","\n","        super(BahdanauAttention, self).__init__() #상위 클래스의 생성자 호출\n","\n","        self.W1 = tf.keras.layers.Dense(units) #units 값 개수만큼의 유닛을 가지는 Dense Layer 생성\n","        # W1 레이어는 attention score를 계산하기 전 쿼리 벡터들이 모두 같은 차원수를 가지도록 변환해주는 역할\n","        self.W2 = tf.keras.layers.Dense(units) #units 값 개수만큼의 유닛을 가지는 Dense Layer 생성\n","        # W2 레이어는 attention score를 계산하기 전 인코더 출력값들이 모두 같은 차원수를 가지도록 변환해주는 역할\n","        self.V = tf.keras.layers.Dense(1) #한 개의 unit을 갖는 Dense Layer 생성.\n","        # V 레이어는 attention score를 계산 후 softmax을 취해 attention weight(어텐션 가중치)을 얻는 역할\n","\n","    def call(self, query, values):\n","        #호출자 파라미터 query: 현재 time step에서의 쿼리 벡터. (seq2seq 모델에서 보통 디코더의 마지막 상태)\n","        #호출자 파라미터 values: value값(인코더에서 모든 time step에서의 output)\n","\n","        #query 벡터에 time axis를 추가한다. 이는 query의 shape을 values의 shape과 맞추어 주기 위함\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","\n","        #어텐션 스코어 계산. 각각 W1,W2를 이용해 query와 values 값을 변환하고 그 둘을 더한 다음,\n","        #hyperbolic tangent 활성화 함수를 적용한 결과를 V에 통과시킨다.\n","        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n","\n","        #softmax 함수(모든 가중치값의 합 1 되게끔)를 적용해 attention weights 계산.\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        #attention weights와 values를 element-wise multiplication 통해 컨텍스트 벡터 계산.(weighted sum)\n","        context_vector = attention_weights * values\n","\n","        #최종 컨텍스트 벡터를 얻기 위해 시간 축을 기준으로 가중치 값 합산. 이는 어텐션 메커니즘을 기반으로\n","        #입력 시퀀스에서 관련 정보를 캡처하는 벡터이다.\n","        #x = tf.constant([[1,1,1],[1,1,1]])\n","        #tf.reduce_sum(x) -> 6\n","        #tf.reduce_sum(x,axis=0) -> [2,2,2]\n","        #tf.reduce_sum(x,axis=1) -> [3,3]\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        #context vector와 attention weights를 반환.\n","        return context_vector, attention_weights\n","\n","class Decoder(tf.keras.Model): #tf.keras.Model 클래스를 상속받아 Decoder Class 구현\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        # 1) vocab_size : 어휘 수(target 랭귀지의)\n","        # 2) embedding_dim : 임베딩 공간의 차원 값(target 언어에서의 각 단어는 해당 차원의 밀집벡터로 표현된다)\n","        # 3) dec_units : 디코더의 GRU 게이어 유닛(뉴런) 개수.\n","        # 4) batch_sz : 배치 사이즈\n","\n","        super(Decoder, self).__init__() #상속받은 상위 클래스의 생성자 호출\n","\n","        #attribute 값 설정\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","\n","        # 디코더 임베딩 레이어 생성.토큰화된 input sequence를 고정 길이(embedding_dim)의 밀집 벡터로 바꾸어 준다\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","\n","        # RNN 레이어(GRU) 생성 - dec_units 개의 유닛을 가지고 return_sequence, return_state를 모두 True로 설정해 줌으로써\n","        # 각 input sequence에 대한 전체 출력 시퀀스, 디코더의 마지막 상태도 받도록.\n","        # GRU 레이어의 가중치 초깃값을 Glorot uniform 으로 설정\n","        self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","\n","        # Fully connected layer 생성 <- target vocabulary 상 확률 분포를 얻기 위함\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","        # BahdanauAttention 클래스의 인스턴스를 생성하여 어텐션 메커니즘을 이용해 디코더가 디코딩 과정에서 그때그때 다른\n","        # input sequence의 부분에 더 집중할 수 있도록 한다\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","    def call(self, x, hidden, enc_output): #호출자\n","        # x: 현재 time step의 input data(토큰화된 시퀀스)\n","        # hidden: 이전 time step의 GRU 레이어 은닉상태--처음은 인코더에서 마지막 hidden state\n","        # enc_output: input sequence의 정보를 담고 있는 인코더의 출력\n","\n","        # Bahdanau Attention을 통해 context vector와 attention weights를 받는다\n","        context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","        # input data x를 임베딩 레이어에 통과시켜서 밀집 벡터 표현을 얻는다\n","        x = self.embedding(x)\n","        # 디코더의 GRU의 입력으로 쓰기 위해 context vector와 x를 concatenate\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        # x를 GRU 유닛에 통과시켜서 output sequence와 마지막 hidden state를 얻는다\n","        output, state = self.gru(x)\n","        #output 텐서를 reshape. reshape에 -1을 쓰면 다른 shape 값만 제대로 맞추도록 알아서 계산하라는 의미(텐서의 기존 요소들 모두 유지되도록)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        #reshape 된 텐서를 fully connected layer에 통과시켜서 모든 타겟 어휘 상의 확률 분포(output logits)를 얻는다\n","        x = self.fc(output)\n","        # x(output logits), state(final hidden state), attention weights를 반환\n","        #x는 예측된 타겟 시퀀스를 생성하기 위해, state는 다음 time step에 전달되기 위해\n","        return x, state, attention_weights"],"metadata":{"id":"MW9R7BmNahl9","executionInfo":{"status":"ok","timestamp":1714974263512,"user_tz":-540,"elapsed":2,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["vocab_inp_size, vocab_tar_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gl4oKVj6ag9m","executionInfo":{"status":"ok","timestamp":1714974266667,"user_tz":-540,"elapsed":3,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"20300291-3cde-4b60-f94b-850a550ad4da"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7961, 3211)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# encoder\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","# 샘플 입력\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n","\n","# encoder-decoder attention\n","attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n","\n","# decoder\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voQ8ycCsa8Hn","executionInfo":{"status":"ok","timestamp":1714975739959,"user_tz":-540,"elapsed":444,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"1ff94a6c-dfd1-4c28-cbd1-007e519222e7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder output shape: (batch size, sequence length, units) (64, 97, 512)\n","Encoder Hidden state shape: (batch size, units) (64, 512)\n","Attention result shape: (batch size, units) (64, 512)\n","Attention weights shape: (batch_size, sequence_length, 1) (64, 97, 1)\n","Decoder output shape: (batch_size, vocab size) (64, 3211)\n"]}]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n","#Sparse Categorical Crossentropy\n","#from_logits: whether y_pred is expected to be a logits tensor.\n","#ruduction: Type of reduction to apply to the loss. In almost all cases this should be\n","#'sum_over_batch_size'. supported options are {sum/sum over batch size} or None\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    # creates a mask where 'True' indicates the positions of non-padding tokens(where 'real' is not equal to 0),\n","    # and 'False' indicates the positions of padding tokens.\n","    loss_ = loss_object(real, pred)\n","    # calculates the loss between the true labels(real) and the predicted labels(pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    # converts the mask to the same data type as the computed loss. This step is necessary to\n","    # ensure that the mask can be multiplied element-wise with the loss.\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_) #calculates the mean of the masked loss values\n","\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)"],"metadata":{"id":"obH_cK58a8Dn","executionInfo":{"status":"ok","timestamp":1714975760145,"user_tz":-540,"elapsed":801,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def train_step(inp, targ, enc_hidden):\n","  loss = 0\n","  with tf.GradientTape() as tape:\n","      # encoder\n","      enc_output, enc_hidden = encoder(inp, enc_hidden)\n","      dec_hidden = enc_hidden\n","\n","      # decoder\n","      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","      # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.\n","      for t in range(1, targ.shape[1]):\n","          # enc_output를 디코더에 전달합니다.\n","          predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","          loss += loss_function(targ[:, t], predictions)\n","\n","          # 교사 강요(teacher forcing)를 사용합니다.\n","          dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","  #trainable_variables는 Encoder와 Decoder가 상속받은 클래스에 있는 속성\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","  return batch_loss"],"metadata":{"id":"oSfZXn8Wa7-2","executionInfo":{"status":"ok","timestamp":1714975762463,"user_tz":-540,"elapsed":2,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# 이 파트가 매우 오래걸림\n","\n","EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","      batch_loss = train_step(inp, targ, enc_hidden)\n","      total_loss += batch_loss\n","\n","      if batch % 10 == 0:\n","          print('Epoch {} / Batch {} / Loss {:.4f} / Time taken {} sec'.format(epoch + 1,\n","                                                                                batch,\n","                                                                                batch_loss.numpy(),\n","                                                                                time.time() - start))\n","  # 에포크가 2번 실행될때마다 모델 저장 (체크포인트)\n","  #if (epoch + 1) % 2 == 0:\n","  #    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} / Loss {:.4f}'.format(epoch + 1,\n","                                        total_loss / steps_per_epoch))\n","  print('Time taken for {} epoch {} sec\\n'.format(epoch + 1,\n","                                                  time.time() - start))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tIL1ky8ua77-","executionInfo":{"status":"ok","timestamp":1714977978940,"user_tz":-540,"elapsed":2212970,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"49087060-f676-49f0-a5a6-2d545d28a010"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 / Batch 0 / Loss 0.5396 / Time taken 5.254581451416016 sec\n","Epoch 1 / Batch 10 / Loss 0.3737 / Time taken 36.35806369781494 sec\n","Epoch 1 / Batch 20 / Loss 0.3100 / Time taken 68.02773213386536 sec\n","Epoch 1 / Batch 30 / Loss 0.3192 / Time taken 99.52334070205688 sec\n","Epoch 1 / Batch 40 / Loss 0.3230 / Time taken 129.43339610099792 sec\n","Epoch 1 / Batch 50 / Loss 0.3345 / Time taken 159.61226558685303 sec\n","Epoch 1 / Batch 60 / Loss 0.3251 / Time taken 189.63830423355103 sec\n","Epoch 1 / Batch 70 / Loss 0.3111 / Time taken 219.1575050354004 sec\n","Epoch 1 / Loss 0.3420\n","Time taken for 1 epoch 225.35382556915283 sec\n","\n","Epoch 2 / Batch 0 / Loss 0.2680 / Time taken 2.8483335971832275 sec\n","Epoch 2 / Batch 10 / Loss 0.2833 / Time taken 31.781784296035767 sec\n","Epoch 2 / Batch 20 / Loss 0.3121 / Time taken 60.94785189628601 sec\n","Epoch 2 / Batch 30 / Loss 0.2660 / Time taken 92.03098678588867 sec\n","Epoch 2 / Batch 40 / Loss 0.2755 / Time taken 121.17725443840027 sec\n","Epoch 2 / Batch 50 / Loss 0.2783 / Time taken 150.7984049320221 sec\n","Epoch 2 / Batch 60 / Loss 0.2562 / Time taken 180.83472156524658 sec\n","Epoch 2 / Batch 70 / Loss 0.2842 / Time taken 210.93130564689636 sec\n","Epoch 2 / Loss 0.2845\n","Time taken for 2 epoch 216.7477581501007 sec\n","\n","Epoch 3 / Batch 0 / Loss 0.2318 / Time taken 3.8107829093933105 sec\n","Epoch 3 / Batch 10 / Loss 0.2512 / Time taken 33.41598916053772 sec\n","Epoch 3 / Batch 20 / Loss 0.2645 / Time taken 63.97862005233765 sec\n","Epoch 3 / Batch 30 / Loss 0.2688 / Time taken 93.53978061676025 sec\n","Epoch 3 / Batch 40 / Loss 0.2536 / Time taken 123.43315649032593 sec\n","Epoch 3 / Batch 50 / Loss 0.2440 / Time taken 153.46878027915955 sec\n","Epoch 3 / Batch 60 / Loss 0.2454 / Time taken 182.418461561203 sec\n","Epoch 3 / Batch 70 / Loss 0.2626 / Time taken 212.11803007125854 sec\n","Epoch 3 / Loss 0.2590\n","Time taken for 3 epoch 217.87926530838013 sec\n","\n","Epoch 4 / Batch 0 / Loss 0.2163 / Time taken 2.70635986328125 sec\n","Epoch 4 / Batch 10 / Loss 0.2496 / Time taken 32.512027978897095 sec\n","Epoch 4 / Batch 20 / Loss 0.2404 / Time taken 62.38380432128906 sec\n","Epoch 4 / Batch 30 / Loss 0.2496 / Time taken 91.25437068939209 sec\n","Epoch 4 / Batch 40 / Loss 0.2436 / Time taken 120.33282709121704 sec\n","Epoch 4 / Batch 50 / Loss 0.2438 / Time taken 149.46472811698914 sec\n","Epoch 4 / Batch 60 / Loss 0.2275 / Time taken 179.7116949558258 sec\n","Epoch 4 / Batch 70 / Loss 0.2438 / Time taken 208.51675271987915 sec\n","Epoch 4 / Loss 0.2413\n","Time taken for 4 epoch 213.87307357788086 sec\n","\n","Epoch 5 / Batch 0 / Loss 0.2592 / Time taken 3.601531744003296 sec\n","Epoch 5 / Batch 10 / Loss 0.2748 / Time taken 33.69184947013855 sec\n","Epoch 5 / Batch 20 / Loss 0.2221 / Time taken 62.462029218673706 sec\n","Epoch 5 / Batch 30 / Loss 0.2426 / Time taken 91.49512195587158 sec\n","Epoch 5 / Batch 40 / Loss 0.2504 / Time taken 121.19307327270508 sec\n","Epoch 5 / Batch 50 / Loss 0.2168 / Time taken 150.4615867137909 sec\n","Epoch 5 / Batch 60 / Loss 0.2515 / Time taken 179.5178201198578 sec\n","Epoch 5 / Batch 70 / Loss 0.2145 / Time taken 210.11905026435852 sec\n","Epoch 5 / Loss 0.2266\n","Time taken for 5 epoch 216.13137483596802 sec\n","\n","Epoch 6 / Batch 0 / Loss 0.2090 / Time taken 2.725165843963623 sec\n","Epoch 6 / Batch 10 / Loss 0.1890 / Time taken 31.80552053451538 sec\n","Epoch 6 / Batch 20 / Loss 0.2225 / Time taken 61.894280672073364 sec\n","Epoch 6 / Batch 30 / Loss 0.2346 / Time taken 90.8887689113617 sec\n","Epoch 6 / Batch 40 / Loss 0.2085 / Time taken 120.39158797264099 sec\n","Epoch 6 / Batch 50 / Loss 0.2045 / Time taken 149.3997917175293 sec\n","Epoch 6 / Batch 60 / Loss 0.2053 / Time taken 179.3080132007599 sec\n","Epoch 6 / Batch 70 / Loss 0.2255 / Time taken 209.84973049163818 sec\n","Epoch 6 / Loss 0.2128\n","Time taken for 6 epoch 215.25009560585022 sec\n","\n","Epoch 7 / Batch 0 / Loss 0.2073 / Time taken 3.5878798961639404 sec\n","Epoch 7 / Batch 10 / Loss 0.1967 / Time taken 32.73727059364319 sec\n","Epoch 7 / Batch 20 / Loss 0.2139 / Time taken 62.024208545684814 sec\n","Epoch 7 / Batch 30 / Loss 0.1859 / Time taken 90.93750405311584 sec\n","Epoch 7 / Batch 40 / Loss 0.2112 / Time taken 120.82797908782959 sec\n","Epoch 7 / Batch 50 / Loss 0.2016 / Time taken 149.9278221130371 sec\n","Epoch 7 / Batch 60 / Loss 0.2204 / Time taken 179.22981023788452 sec\n","Epoch 7 / Batch 70 / Loss 0.1987 / Time taken 209.0883753299713 sec\n","Epoch 7 / Loss 0.2017\n","Time taken for 7 epoch 215.16784954071045 sec\n","\n","Epoch 8 / Batch 0 / Loss 0.1807 / Time taken 2.717386245727539 sec\n","Epoch 8 / Batch 10 / Loss 0.1895 / Time taken 31.685171842575073 sec\n","Epoch 8 / Batch 20 / Loss 0.1885 / Time taken 61.55118203163147 sec\n","Epoch 8 / Batch 30 / Loss 0.1754 / Time taken 90.44241952896118 sec\n","Epoch 8 / Batch 40 / Loss 0.2014 / Time taken 119.43006420135498 sec\n","Epoch 8 / Batch 50 / Loss 0.1815 / Time taken 148.42149829864502 sec\n","Epoch 8 / Batch 60 / Loss 0.1924 / Time taken 179.36373925209045 sec\n","Epoch 8 / Batch 70 / Loss 0.2004 / Time taken 208.4376721382141 sec\n","Epoch 8 / Loss 0.1913\n","Time taken for 8 epoch 213.85535216331482 sec\n","\n","Epoch 9 / Batch 0 / Loss 0.1644 / Time taken 3.568786859512329 sec\n","Epoch 9 / Batch 10 / Loss 0.1736 / Time taken 32.90928936004639 sec\n","Epoch 9 / Batch 20 / Loss 0.1860 / Time taken 61.943204402923584 sec\n","Epoch 9 / Batch 30 / Loss 0.1609 / Time taken 91.28462338447571 sec\n","Epoch 9 / Batch 40 / Loss 0.1613 / Time taken 121.29462480545044 sec\n","Epoch 9 / Batch 50 / Loss 0.2286 / Time taken 150.66126990318298 sec\n","Epoch 9 / Batch 60 / Loss 0.1778 / Time taken 180.7201840877533 sec\n","Epoch 9 / Batch 70 / Loss 0.1850 / Time taken 210.64005184173584 sec\n","Epoch 9 / Loss 0.1811\n","Time taken for 9 epoch 216.32001948356628 sec\n","\n","Epoch 10 / Batch 0 / Loss 0.1515 / Time taken 2.7216358184814453 sec\n","Epoch 10 / Batch 10 / Loss 0.1650 / Time taken 32.336235761642456 sec\n","Epoch 10 / Batch 20 / Loss 0.1758 / Time taken 62.3652126789093 sec\n","Epoch 10 / Batch 30 / Loss 0.1682 / Time taken 91.5924985408783 sec\n","Epoch 10 / Batch 40 / Loss 0.2220 / Time taken 120.75073504447937 sec\n","Epoch 10 / Batch 50 / Loss 0.1899 / Time taken 150.09231114387512 sec\n","Epoch 10 / Batch 60 / Loss 0.1740 / Time taken 180.3912341594696 sec\n","Epoch 10 / Batch 70 / Loss 0.1673 / Time taken 209.5207085609436 sec\n","Epoch 10 / Loss 0.1732\n","Time taken for 10 epoch 261.91244411468506 sec\n","\n"]}]},{"cell_type":"code","source":["def evaluate(sentence):\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","\n","    sentence = preprocess_sentence(sentence)\n","\n","    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","\n","    result = ''\n","\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                             dec_hidden,\n","                                                             enc_out)\n","\n","        # 나중에 어텐션 가중치를 시각화하기 위해 어텐션 가중치를 저장합니다.\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang.index_word[predicted_id] + ' '\n","\n","        if targ_lang.index_word[predicted_id] == '<end>':\n","            return result, sentence, attention_plot\n","\n","        # 예측된 ID를 모델에 다시 피드합니다.\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return result, sentence, attention_plot"],"metadata":{"id":"XoqHf8Pwa7kd","executionInfo":{"status":"ok","timestamp":1714978074381,"user_tz":-540,"elapsed":2,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def translate(sentence):\n","    result, sentence, attention_plot = evaluate(sentence)\n","\n","    #print('Input: %s' % (sentence))\n","    #print('Predicted translation: {}'.format(result))\n","    return result"],"metadata":{"id":"ssCfwPm7bFe-","executionInfo":{"status":"ok","timestamp":1714978074875,"user_tz":-540,"elapsed":1,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["translate('톰이 마실 수 있는 것은 오직 물 뿐이야')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"X8XuuHl9bGqe","executionInfo":{"status":"ok","timestamp":1714978079154,"user_tz":-540,"elapsed":1248,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"e5ac42cd-2bb0-442d-fcbc-1e310f676255"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"i don't want to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to \""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# BLEU score\n","import collections\n","import math\n","\n","def ngram_counts(sentence, n):\n","  words = sentence.split()\n","  ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n","  return collections.Counter(ngrams)\n","  #ngram 시퀀스마다 갯수를 센 Counter 딕셔너리 반환\n","\n","#문장길이에 대한 과적합 보정\n","def brevity_penalty(pred, real):\n","  p_len = len(pred.split())\n","  r_len = len(real.split())\n","  if p_len > r_len:\n","    return 1\n","  else:\n","    return (p_len/r_len)\n","    #return math.exp(1-r_len/p_len) #min(1, pred len/real len)\n","\n","def bleu_score(pred, real, weights=(0.25, 0.25, 0.25, 0.25)):\n","  bp = brevity_penalty(pred, real)\n","  ng = 1\n","\n","  for i, weight in enumerate(weights, start=1):\n","    #i-gram 1부터 4까지\n","    gram_len = len(pred)-i+1 #분모(예측된 문장에서 n-gram 시퀀스의 길이)\n","    cnt = 0\n","    ngram_cnts_pred = ngram_counts(pred, i)\n","    ngram_cnts_real = ngram_counts(real, i)\n","    for key,value in ngram_cnts_pred.items():\n","      if key in ngram_cnts_real:\n","        # cnt += min(value, ngram_cnts_real[key])\n","        if value > ngram_cnts_real[key]:\n","          cnt += ngram_cnts_real[key] #Clipping\n","        else:\n","          cnt += value\n","    ng *= (cnt / gram_len)**weight\n","\n","  return bp * ng\n"],"metadata":{"id":"_3GDKufCbI8Z","executionInfo":{"status":"ok","timestamp":1714978085471,"user_tz":-540,"elapsed":446,"user":{"displayName":"HJ","userId":"04584162756684325171"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["#Test data로 성능 평가해보기(BLUE)\n","bleu = 0\n","for i, sample in enumerate(input_tensor_val):\n","\n","  inp_sent = ''\n","  for w in sample:\n","    if w!=0:\n","      kw = inp_lang.index_word[w]\n","      if kw != '<start>' and kw != '<end>':\n","        inp_sent = inp_sent + kw +' '\n","  #print(inp_sent)\n","\n","  pred_sent = translate(inp_sent).replace(' <end> ','')\n","\n","  targ_sent = ''\n","  for w in target_tensor_val[i]:\n","    if w!=0:\n","      ew = targ_lang.index_word[w]\n","      if ew != '<start>' and ew != '<end>':\n","        targ_sent = targ_sent + ew + ' '\n","  #print(targ_sent)\n","  bleu += bleu_score(pred_sent, targ_sent)\n","\n","  if i%100 == 0:\n","    print(f'{i+1}samples sampled')\n","\n","print(f\"총합산한 BLEU스코어: {bleu:.10f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"93v35sodbI6L","executionInfo":{"status":"ok","timestamp":1714978537098,"user_tz":-540,"elapsed":441435,"user":{"displayName":"HJ","userId":"04584162756684325171"}},"outputId":"f06b13ae-3753-4f98-c51c-272847a803ee"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["1samples sampled\n","101samples sampled\n","201samples sampled\n","301samples sampled\n","401samples sampled\n","501samples sampled\n","601samples sampled\n","701samples sampled\n","801samples sampled\n","901samples sampled\n","1001samples sampled\n","1101samples sampled\n","총합산한 BLEU스코어: 0.4347332528\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gwUPlfhLbI3n"},"execution_count":null,"outputs":[]}]}